{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bcdb7b8-c3a5-4da0-801b-54b5094a07b1",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "* Goal: Understanding the mappings between constituent and compound embeddings\n",
    "* Why does a linear projection work so well?\n",
    "  * Linear projection: 80K parameters, Test acc: .39\n",
    "  * Best DL model: 7.2M parameters, Test acc: .51\n",
    "* What has the linear projection captured in this task?\n",
    "* How could we model the different meanings of the constituents? (e.g. 土 tǔ shí \"land, clay\" in 土石 tǔ shí \"earth and stoes\" vs. 土狗 tǔ gǒu \"native dogs\")\n",
    "\n",
    "### 2-Char (conv-based/linear/fully-connected layers)    \n",
    "\n",
    "|             | Train  |  Test\n",
    "|------------|--------:|--------:|\n",
    "| conv-based | .78     |   .46   |\n",
    "| linear     | .40     |   .39   |\n",
    "| emb-tuning (init with tencent)  | .83     |   .51   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c5bf84-f808-435f-b674-bdd047119e9c",
   "metadata": {},
   "source": [
    "### Underlying function\n",
    "The x- and y- axis could be considered as a one-dimensinoal constituent embedding. For example, \"土\", \"著\", \"石\" would be at three different locations along the x axis. The warped grid is like the word embeddings. In this example, the relationship between compound ($x$, $y$ space) and constituent ($u$ and $v$ coordinates) embedding is:\n",
    "\n",
    "$$\n",
    "x = u + 0.2 \\cdot sin(v*PI) \\\\\n",
    "y = v + 0.2 \\cdot cos(u*PI)\n",
    "$$\n",
    "\n",
    "![non-linear](img/non-linear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff440ec6-9514-4c3a-b6a7-775cb2f10db4",
   "metadata": {},
   "source": [
    "## A Linear projection\n",
    "\n",
    "The linear projection model estimates a transformation matrix, which, in this case, is a shear matrix.\n",
    "\n",
    "The intuition here is the same character (in the same $u$ location) would have different \"effect\" on the word embeddings (indicated by the black arrows), but they will all be the same in the linear projection. The local \"effect\" of the character is the same with the global linear approximation.\n",
    "\n",
    "If we can model the underlying distribution, and extract the \"effect\" from the model, would that \"effect\" somehow _reflect_ the constituents' meaning?\n",
    "\n",
    "$$\n",
    "M = (U^\\top U)^{-1}U^{\\top} X \\\\\n",
    "\\begin{bmatrix}\n",
    "u & v\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "1.00 & 0.01 \\\\ 0.19 & 1.00\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "u+0.19v & 0.01u+v\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "![linear](img/linear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2b1b0f-babb-467c-908f-f8d5723208c1",
   "metadata": {},
   "source": [
    "## Approximating with the deep learning model\n",
    "\n",
    "A deep learning model better approximate the underlying non-linear relations. Further more, we could similary derive the \"effect\" of the constituent in that specific context with Jacobian matrix of the DL model. \n",
    "\n",
    "As long as the model approximates the underlying function, the \"effect\" of the constituent is similar.\n",
    "\n",
    "![dl.png](img/dl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd7cfa1-3384-46f4-aa6f-815a0cfe9f7a",
   "metadata": {},
   "source": [
    "## Empirical study\n",
    "\n",
    "* Show the model predict real words well\n",
    "* Show the model predict pseudowords well: help account for the non-word behavioral data (MELD-SCH)\n",
    "* Show that the character Jacobian (Jacobian matrix of that constituents) reflect the character's meanings in the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da763f4b-ddd0-4ffb-be8d-f69a72dc72a6",
   "metadata": {},
   "source": [
    "### Real word embedding predictions\n",
    "<img src=\"img/morphert-acc.png\" alt=\"drawing\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bf5721-563c-461c-a834-3774611b7068",
   "metadata": {},
   "source": [
    "### Pseudowords: MELD-SCH data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab9fc7-8ea8-44fe-b013-119c92172cf7",
   "metadata": {},
   "source": [
    "<img src=\"80.11-nw-paper-figure.png\" alt=\"80.11-nw-paper-figure\" style=\"width:800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15549526-b59f-4172-9cc2-539d811e5a68",
   "metadata": {},
   "source": [
    "### Character meaning clustering based on Character Jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa7ccd2-de06-4868-831e-0db073015377",
   "metadata": {},
   "source": [
    "<img src=\"30.22-affix-series.png\" alt=\"30.22-affix-series\" style=\"width:800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f43574-2b1a-4172-8667-f43a95d4642f",
   "metadata": {},
   "source": [
    "<img src=\"30.22-affix-pval-distr.png\" alt=\"30.22-affix-pval-distr\" style=\"width:800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f1dbf6-cc51-4b18-acb9-64c3eb01ed6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
